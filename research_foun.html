<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="icon" href="cty.png" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Tianyi Chen 陈天翼</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="people.html">People</a></div>  	
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>    	
<div class="menu-item"><a href="experience.html">Experience</a></div>
<div class="menu-item"><a href="awards.html">Awards</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Optimization foundation of machine learning</h1>
<div id="subtitle"></div>
</div>
<!-- <h2>Research Overview</h2> -->
<div id="main">
	<br>
	<figure style="float:right; 
	 margin:0px 0px 15px 15px; 
	 padding: 0px;
	text-align: justify;">
    <img height="350" width="450" src="figures/optview.png" alt="" title="" > 
		<p></p>		
    <figcaption>
	<strong>
	</strong> 
	</figcaption>
	</figure>
<script type="text/javascript">
    function toggleAbstract(divid) {
      var x = document.getElementById(divid);
      if (x.style.display === "none") {
          x.style.display = "block";
      } else {
          x.style.display = "none";
      }
    }
  </script>	
	
<p>The past decade has witnessed successful applications of artificial intelligence (AI) and machine learning (ML) in almost every branch of science and engineering. The driving force of this success is that optimization is becoming the enabling factor of modern AI and ML applications. However, the efficiency of these optimization methods remains far from full satisfaction to meet the evergrowing demand that arises in new AI/ML applications in many real-world settings.</p>
  

<h3>Bilevel optimization for learning with nested structures</h3>
<p>Many ML problems today, such as meta learning, hyper-parameter optimization, and reinforcement learning, go beyond the above simple minimization structure, and have hierarchically coupled structures (termed the nested learning thereafter). As a result, computing (stochastic) gradients, i.e., the basic elements in first-order optimization machinery, becomes prohibitively expensive and even impossible in some situations. In this context, we develop new computational methods to address this issue. Our paper that includes the initial efforts in this direction was recognized as the <b>ICASSP Outstanding Student Paper Award</b>. </p>
	
<!-- <p><table class="imgtable"><tr><td>
<img src="figures/fedcomp.png" alt="need photo here" width="350" height="230" />&nbsp&nbsp&nbsp&nbsp&nbsp <img src="figures/fedcomp1.png" alt="need photo here" width="650" height="230" /></td>
<td align="left"></td></tr></table></p> -->
	
<p><b>Related publications</b>: </p>
<ol>	
<p><li>T. Chen, Y. Sun and W. Yin, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/2008.10847.pdf">Solving stochastic compositional Optimization is nearly as easy as solving stochastic optimization</a>,’’ <i>IEEE Transactions on Signal Processing</i> (<tt>TSP</tt>), vol. 69, pp. 4937 - 4948, June 2021. </li> </p>
	
<p><li>T. Chen, Y. Sun, and W. Yin, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/2106.13781.pdf">Tighter analysis of alternating stochastic gradient method for stochastic nested problems</a>,’’ submitted to <i>Proc. of Neural Information Processing Systems</i> (<tt>NeurIPS</tt>), virtual, December 3-8, 2021.</li> </p>	

<p><li>T. Chen, Y. Sun, and W. Yin, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/2102.04671.pdf">A single-timescale method for stochastic bilevel optimization</a>,’’ submitted to <i>Proc. of Neural Information Processing Systems</i> (<tt>NeurIPS</tt>), virtual, December 3-8, 2021.</li></p> 

</ol>

<h3>Distributed optimization in resource-limited regimes</h3>	
<p>State-of-the-art deep neural network models and contemporary ML algorithms tend to be resource-intensive, often requiring a large amount of data and significant computing power. The efficiency of optimization methods remains far from full satisfaction to meet the evergrowing demand of new AI/ML applications in resource-limited regimes. Resource intensity is a more severe issue when running ML applications in a distributed manner at wirelessly connected devices. In this context, we develop new distributed optimization methods to address this issue. <br />
See more background and detials in <a href="research_dml.html"><b>distributed machine learning</b></a>.	</p>
	
<p><b>Related publications</b>: </p>
<ol>	
<p><li>T. Chen, G. B. Giannakis, T. Sun and W. Yin, &lsquo;&lsquo;<a href="http://papers.nips.cc/paper/7752-lag-lazily-aggregated-gradient-for-communication-efficient-distributed-learning.pdf">LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning</a>,’’ <i>Proc. of Neural Information Processing Systems</i> (<tt>NeurIPS</tt>), Montreal, Canada, December 3-8, 2018. <a href="https://nips.cc/Conferences/2018/Schedule?showEvent=12765"><b>(Spotlight talk</b></a> and <a href="https://www.dropbox.com/s/m80ytd2kqn4b72p/poster_nips.pdf?dl=0"><b>poster)</b></a> </li> </p>

<p><li>J. Sun, T. Chen, G. B. Giannakis, and Z. Yang, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1909.07588.pdf">Communication-Efficient Distributed Learning via Lazily Aggregated Quantized Gradients</a>,’’ <i>Proc. of Neural Information Processing Systems</i> (<tt>NeurIPS</tt>), Vancouver, Canada, December 3-8, 2019.</li> </p>	
	
<p><li>T. Chen, Y. Sun, and W. Yin, &lsquo;&lsquo;<a href="https://ieeexplore.ieee.org/abstract/document/9497717">Communication-Adaptive Stochastic Gradients for Distributed Learning</a>,’’ <i>IEEE Transactions on Signal Processing</i> (<tt>TSP</tt>), vol. 69, pp. 4637 - 4651, July 2021. </li> </p>

</ol>	
 
<h3>Online optimization with complex environments</h3>	
<p>Online convex optimization, belonging to the class of time-varying optimization, is an emerging
methodology for sequential inference with well documented merits. However, early works in this area rarely consider complex environments, which prevents one from implementing them in challenging application domains. These complex environments include settings where i) constraints can possibly be satisfied in the long term rather than a per-round basis; ii) online possibly bandit feedbacks incur unknown delay that prevents one from updating variables in a timely manner; and, iii) the optimization variable is a nonlinear function rather than a finite-dimensional vector. My research in this direction focuses on generalizing existing online optimization approaches to those challenging environments.</p>
<!-- <p><table class="imgtable"><tr><td>
<img src="figures/AdaRaker.png" alt="need photo here" width="450" height="250" />&nbsp;</td>
<td align="left"></td></tr></table></p> -->
<p><b>Related publications</b>: </p>
<ol>	
<p><li>T. Chen, Q. Ling and G. B. Giannakis, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8442.pdf">An Online Convex Optimization Approach to Proactive Network Resource Allocation</a>,’’ <i>IEEE Transactions on Signal Processing</i> (<tt>TSP</tt>), vol. 65, no. 24, pp. 6350-6364, Dec. 2017. </li></p>

<p><li>B. Li, T. Chen, and G. B. Giannakis, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1807.03205.pdf">Bandit Online Learning with Unknown Delays,</a>'' <i>Proc. of Intl. Conf. on Artificial Intell. and Stat.</i> (<tt>AISTATS</tt>), Naha, Japan, April 2019. </li></p>
	
<p><li>Y. Shen, T. Chen, and G. B. Giannakis, &lsquo;&lsquo;<a href="jmlr2019_vfinal.pdf">Random feature-based online multi-kernel learning in environments
with unknown dynamics</a>'' <i>Journal of Machine Learning Research</i> (<tt>JMLR</tt>), vol. 20, no. 22, pp. 1-36, February 2019. Also accepted in part to <a href="http://proceedings.mlr.press/v84/shen18a/shen18a.pdf">AISTATS 2018</a>.</li></p>
</ol>



<div id="footer">
<div id="footer-text">
Page generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
