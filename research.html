<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="icon" href="cty.png" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Tianyi Chen 陈天翼</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="experience.html">Experience</a></div>
<div class="menu-item"><a href="awards.html">Awards</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research</h1>
<div id="subtitle"></div>
</div>
<h2>Research Overview</h2>    
	<div id="main">
	<br>
	<figure style="float:right; 
	 margin:0px 0px 15px 15px; 
	 padding: 0px;
	text-align: justify;">
    <img height="350" width="485" src="figures/resview.png" alt="" title="" > 
<!-- 		    <img height="270" width="485" src="figures/overview.png" alt="" title="" >  -->
		<p></p>		
    <figcaption>
	<strong>
<!-- 		Machine Learning and Optimization for Internet-of-Things  -->
	</strong> 
	</figcaption>
	</figure>
<script type="text/javascript">
    function toggleAbstract(divid) {
      var x = document.getElementById(divid);
      if (x.style.display === "none") {
          x.style.display = "block";
      } else {
          x.style.display = "none";
      }
    }
  </script> 
<p>The past decade has witnessed a proliferation of connected devices and objects, where the notion
of Internet-of-Things (IoT) plays a central role in the envisioned technological advances. Conceptually
speaking, IoT foresees an intelligent system with ubiquitous smart devices. Today, a number
of IoT applications have already brought major benefits to many aspects of our daily life. 
At the same time, the proliferation of machine learning advances motivates a systematic way to uncover hidden insights through learning from historical relationships and trends in massive datasets. Integration of data analytics into future system design holds the key to exploiting the potential of IoT.
In this context, I work at the intersection of <b>optimization</b>, <b>machine learning</b>, and <b>networked systems</b>. My background is in optimization, statistical
signal processing, machine learning, and wireless communications. My current
research focuses on building fundamental connections between methodologies from the optimization, machine
learning and networking communities, and developing inter-disciplinary approaches for networked systems such as the emerging IoT paradigm.
</p>
  
<p>My research has contributed answers to the following <b>two intertwined questions</b>.</p>
  
<p><b>(Q1)</b>: How can we scale up machine learning approaches for efficient IoT implementation?<br />
<b>(Q2)</b>: How learning advances can be leveraged to enhance resource allocation policies for IoT?</p>
  
<p>The overarching objective of my research is to wed state-of-the-art optimization and machine learning
tools with the emerging IoT paradigm, in a way that they can inspire and reinforce the development
of each other, with the ultimate goal of benefiting our daily life.</p>
  
<!-- <h2>Current Research</h2> -->
<p>In the pursuit of the research goals, my research can be summarized into two major areas.</p>
		
<h2>Scale Up Machine Learning Approaches for IoT</h2>
<p>It is estimated that by 2020, there will be more than 50 billion devices connected through the Internet.
To tackle (Q1), it is evident that scalability and heterogeneity are two key challenges for IoT.
Scalability is not only about computational efficiency, but also about communication overhead of
running learning algorithms at the network edge; while heterogeneity comes from both the wide
range of hardware devices, as well as the diversity of IoT tasks offered by each device. Results in this line of research have also been presented as a part of <a href="https://events.afcea.org/MILCOM18/Public/SessionDetails.aspx?FromPage=Sessions.aspx&SessionID=6712&SessionDateID=518">a tutorial</a> we delivered at MILCOM 2018.</p>
  
<!-- <h4> <li>Federated learning at the network edge</li> </h4> -->
<h3>Federated learning at the network edge</h3>
Conventional machine learning approaches require centralizing the users’ data in a remote data center, which is known as cloud computing. In cloud computing, users’ data from the devices, such as mobile phones, are transferred to the data centers which execute learning algorithms on CPU and GPU clusters. 
The extracted information is then transferred back to users’ devices.
Considering the massive amount of IoT devices, centralized learning becomes computationally intractable, and rises
serious privacy concerns. To date, the widespread consensus is that besides data centers at the
cloud, future machine learning tasks have to be performed starting from the network edge, namely
mobile devices. This is the overarching goal of <a href="https://www.youtube.com/watch?v=1vFdnhsI1Nk">edge computing</a>, also known as <a href="https://youtu.be/gbRJPa9d-VU">federated learning</a>. Towards this goal, my research efforts are centered on reducing the communication overhead
during the federated learning processes, and enhancing the robustness of learning under adversarial
(a.k.a. Byzantine) attacks. Relevant results have been published in top machine learning venues (NeurIPS
2018 and AAAI 2019). It is worth mentioning that our learning methods (<tt>LAG-WK</tt> and <tt>LAG-PS</tt>) with adaptive communication
mechanism have been selected as the spotlight presentation in NeurIPS, which establishe
a provably reduced communication complexity in federated learning. Challenges of distributed
learning also lie in asynchrony and delay introduced by e.g., IoT mobility and heterogeneity. In this
context, we have developed algorithms for delayed online learning that can be run asynchronously
on edge devices; see our recent submission to AISTATS.</p>
<p><table class="imgtable"><tr><td>
<img src="figures/fedcomp.png" alt="need photo here" width="450" height="260" />&nbsp&nbsp&nbsp&nbsp&nbsp <img src="figures/fedcomp1.png" alt="need photo here" width="700" height="250" /></td>
<td align="left"></td></tr></table></p>
<p><b>Related publications</b>: </p>
<ol>
<li><p><b>T. Chen</b>, G. Giannakis, T. Sun and W. Yin, &lsquo;&lsquo;<a href="http://papers.nips.cc/paper/7752-lag-lazily-aggregated-gradient-for-communication-efficient-distributed-learning.pdf">LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning</a>,’’ <i>Proc. of Neural Information Processing Systems</i> (NeurIPS), Montreal, Canada, December 3-8, 2018. <a href="https://nips.cc/Conferences/2018/Schedule?showEvent=12765"><b>(Spotlight talk</b></a> and <a href="https://www.dropbox.com/s/m80ytd2kqn4b72p/poster_nips.pdf?dl=0"><b>poster)</b></a> </p>
</li> 
<li><p>L. Li, W. Xu, <b>T. Chen</b>, G. Giannakis, and Q. Ling, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1811.03761.pdf">RSA: Byzantine-Robust Stochastic Aggregation Methods for Distributed Learning from Heterogeneous Datasets,’’</a> <i>Proc. of the Assoc. for the Advanc. of Artificial Intelligence </i> (AAAI), Honolulu, Hawai, January 27-February 1, 2019. (Oral presentation)</p> 
</li>
<li><p>B. Li, <b>T. Chen</b>, and G. Giannakis, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1807.03205.pdf">Bandit Online Learning with Unknown Delays,</a>'' <i>Proc. of Intl. Conf. on Artificial Intell. and Stat.</i> (AISTATS), Naha, Japan, April 2019. </p>
</li>
</ol>

<h3>Federated reinforcement learning over networked agents</h3>
<!-- <h4> <li>Federated reinforcement learning over networked agents</li> </h4> -->
<p>From learning to control, reinforcement learning (RL) will play a critical role in many complex IoT tasks. Popular RL algorithms are originally developed for the single-agent tasks, but a number of IoT tasks such as autonomous vehicles, coordination of unmanned aerial vehicles (UAV), involve multiple agents operating in a distributed fashion. Today, a group of coordinated UAVs can perform traffic control, food delivery, rescue and search tasks. To coordinate agents distributed over a network however, information exchange is necessary, which requires frequent communication among agents. 
For resource-limited devices (e.g., battery-powered UAVs), communication is costly and the latency caused by frequent communication becomes the bottleneck of overall performance. In this context, we have studied the distributed RL (DRL) problem that covers multi-agent collaborative RL and parallel RL. 
Generalizing our theory and algorithms for supervised learning, we have developed an exciting communication-efficient algorithm (<tt>LAPG</tt>) for DRL, which builds on the policy gradient (PG) method. Remarkabley, our new method can achieve the same order of convergence rates as vanilla policy gradient under standard conditions; and, ii) reduce the communication rounds required to achieve
a targeted learning accuracy, when the distributed agents are heterogeneous. </p>
	
<p><table class="imgtable"><tr><td>
<img src="figures/DRLtask.png" alt="need photo here" width="450" height="250" />&nbsp&nbsp&nbsp&nbsp <img src="figures/DRLtask1.png" alt="need photo here" width="700" height="250" /></td>
<td align="left"></td></tr></table></p>
	
<p><b>Related publications</b>: </p>
<ol>
<li><p><b>T. Chen</b>, K. Zhang, G. B. Giannakis, and T. Ba&#351;ar, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1812.03239.pdf">Communication-Efficient Distributed Reinforcement Learning,’’</a> Submitted to  <i>Journal of Machine Learning Research</i>, December 2018. </p>
</li>  
</ol>
  
<!-- <h4><li>Scalable function approximation with unknown dynamics</li> </h4> -->
<h3>Scalable function approximation with unknown dynamics</h3>	
Function approximation emerges at the core of machine learning tasks such as regression, classification, dimensionality reduction, as well as reinforcement learning. Kernel methods exhibit well-documented performance in function
approximation. However, the major challenges of implementing existing methods to IoT come
from two sources: i) the “curse” of dimensionality in kernel-based learning; and, ii) the need to
track time-varying functions with unknown dynamics. In this context, we have developed a scalable
multi-kernel learning scheme to obtain the sought nonlinear learning function ‘on the fly.’
To further boost performance in unknown environments, we also developed an adaptive learning
scheme, which accounts for the unknown dynamics. So far, results in this direction have appeared
in the top artificial intelligence (AI) venue AISTATS and the flagship journal JMLR. Our
scalable algorithm is uniquely capable of tracking learning functions in IoT environments with unknown
dynamics, and with analytical as well as empirical performance guarantees. </p>
<!-- <p><table class="imgtable"><tr><td>
<img src="figures/AdaRaker.png" alt="need photo here" width="450" height="250" />&nbsp;</td>
<td align="left"></td></tr></table></p> -->
<p><b>Related publications</b>: </p>
<ol>
<li><p>Y. Shen*, <b>T. Chen</b>*, and G. B. Giannakis, &lsquo;&lsquo;<a href="http://proceedings.mlr.press/v84/shen18a/shen18a.pdf">Online Ensemble Multi-Kernel Learning Adaptive to Nonstationary and Adversarial Environments</a>,’’ <i>Proc. of Intl. Conf. on Artificial Intell. and Stat.</i> (AISTATS), Lanzarote, Canary Islands, April 2018. (equal contribution) </p>
</li>
<li><p>Y. Shen, <b>T. Chen</b>, and G. B. Giannakis, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1712.09983.pdf">Random Feature-Based Online Multi-Kernel Learning in Environments
with Unknown Dynamics</a>'' <i>Journal of Machine Learning Research</i>, conditional accepted, October 2018.</p>
</li>
</ol>
  
<h2>Rethink Resource Management for IoT via Machine Learning</h2>
<p>Optimally allocating limited computing and communication resources is a crucial task in resource-limited IoT environments. To
tackle <b>(Q2)</b>, I have provided affirmative answers to the following intermediate questions:</p> 
  
<p><b> i)</b> can we learn from historical data to improve the existing resource management schemes; and,<br /> 
<b>ii)</b> can we develop resource management schemes when the underlying models are not known? <br /> </p>
  
<p>The key novelty here is innovative statistical and interactive learning tailored for resource management tasks in IoT. Results in this direction have been disseminated as part of <a href="http://globecom2018.ieee-globecom.org/program/tutorials#tut09">a tutorial</a> we co-presented at GLOBECOM 2018, and summarized in our <b>overview paper</b>.</p>
  
<!-- <p><b>Overview paper</b>:</p> -->
<p><b>T. Chen</b>, S. Barbarossa, X. Wang, G. Giannakis and Z.-L. Zhang, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1810.11613.pdf">Learning and Management for Internet-of-Things: Accounting for Adaptivity and Scalability</a>,’’ <i> Proceedings of the IEEE</i>, to appear, 2018.
 </p>

<!-- <h4><li>Statistical learning viewpoint of resource management</li></h4> -->
<h3>Statistical learning viewpoint of resource management</h3>	
To date, most resource management schemes for IoT are based on a pure optimization viewpoint (e.g., the dual (sub)gradient method), which incur large queueing delays and slow convergence. From the vantage point of IoT, our fresh
idea is to leverage the abundant historical data collected by devices, and formulate the resource
management problem as an empirical risk minimization (ERM) — a central topic of statistical machine
learning research. In this context, we have developed a fast convergent algorithm. By
cross-fertilizing advances of learning theory, we have also established the sample complexity of
learning a near-optimal resource management policy. To boost performance in dynamic settings,
we further introduced a learn-and-adapt resource management framework (<tt>LA-SDG</tt>), which capitalizes
on the following features: (f1) it learns from historical data using advanced statistical learning
tools; and, (f2) it efficiently adapts to IoT dynamics, and thus enables operational flexibility. Our
proposed algorithms have been published in top signal processing and network optimization
journals, where we have analytically shown that this novel algorithmic design can provably
improve the emerging performance tradeoff by an order of magnitude. Preliminary results in this
direction have also played a key role in helping our group receive the NSF CCSS grant. To demonstrate
the impact of this work, we have applied it to mobile computing and smart grid tasks.</p>
<p><table class="imgtable"><tr><td>
<img src="figures/statlearn.png" alt="need photo here" width="540" height="220" />&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<img src="figures/statlearn1.png" alt="need photo here" width="630" height="220" /></td>
<td align="left"></td></tr></table></p>
  
<p><b>Recent publications</b>:</p>
<ol>
<li><p><b>T. Chen</b>, A. Mokhtari, X. Wang, A. Ribeiro and G. Giannakis, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8422.pdf">Stochastic Averaging for Constrained Optimization with Application to Online Resource Allocation</a>,’’ <i>IEEE Transactions on Signal Processing</i>, vol. 65, no. 12, pp. 3078-3093, Jun. 2017. </p>
</li>
<li><p><b>T. Chen</b>, Q. Ling and G. Giannakis, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8437.pdf">Learn-and-Adapt Stochastic Dual Gradients for Network Resource Allocation</a>,’’<i>IEEE Transactions on Control of Network Systems</i>, vol. 5, no. 4, pp. 1941-1951, December 2018. </p>
</li>
<li><p>B. Li, <b>T. Chen</b>, X. Wang and G. Giannakis, &lsquo;&lsquo;<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8214260">Real-time Optimal Energy Management with Reduced Battery Capacity Requirements</a>,’’ <i>IEEE Transactions on Smart Grid</i>, to appear 2019. </p>
</li> 
<li><p>X. Chen, W. Ni, <b>T. Chen</b>, I. Collins, X. Wang and G. Giannakis, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1804.07051.pdf">Multi-Timescale Optimization of
Network Function Virtualization for Service Chaining</a>,’’ <i>IEEE Transactions on Mobile Computing</i>, to appear 2019. </p>
</li>   
</ol>

<!-- <h4><li>Model-free interactive management for edge computing</li></h4> -->
<h3>Model-free interactive management for edge computing</h3>
Typically, solving resource allocation
problems necessitates knowledge of the models that map a resource allocation decision to its cost
or utility; e.g., the model that maps transmit-power to the bit rate in communication systems. However,
such models may not be available in IoT, because i) the utility function capturing e.g., service
latency or reliability in edge computing, can be hard to model; and, ii) even if modeling is possible,
IoT devices with limited resources may not afford the complexity of running sophisticated inference
algorithms. Hence, another important ingredient of my research is to account for the feedback
limited nature of resource allocation tasks in IoT. To account for physical constraints, we have considerably
generalized the interactive learning tools for unconstrained problems to solve challenging constrained resource allocation problems. Tailored for edge computing scenarios, we further developed a class of model-free online learning schemes. Our new algorithms (<tt>BanSaP</tt>) come with provable performance guarantees, even when knowledge about the underlying system models can be obtained only through repeated interactions with the environment. Results in this
direction were among the top 10 nominated for the best paper award in the 2017 IEEE Asilomar conference.</p>
<p><table class="imgtable"><tr><td>
<img src="figures/modfree.png" alt="need photo here" width="450" height="240" />&nbsp&nbsp&nbsp&nbsp&nbsp <img src="figures/modfree1.png" alt="need photo here" width="720" height="240" /></td>
<td align="left"></td></tr></table></p>
<p><b>Related publications</b>:</p>
<ol>
<li><p><b>T. Chen</b>, Q. Ling and G. Giannakis, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8442.pdf">An Online Convex Optimization Approach to Proactive Network Resource Allocation</a>,’’ <i>IEEE Transactions on Signal Processing</i>, vol. 65, no. 24, pp. 6350-6364, Dec. 2017. </p>
</li>
<li><p><b>T. Chen</b>, Q. Ling, Y. Shen and G. Giannakis, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8498.pdf">Heterogeneous Online Learning for Thing-Adaptive Low-Latency Fog Computing in IoT</a>,’’ <i>IEEE Internet of Things Journal</i>, to appear 2018.</p>
</li>
<li><b>T. Chen</b> and G. Giannakis, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8470.pdf">Bandit Convex Optimization for Scalable and Dynamic IoT Management</a>,’’ <i>IEEE Internet of Things Journal</i>, to appear 2018. </p>
</li>
</ol>
  
<!-- <h4><li>Stochastic optimization for networked cyber-physical systems</li></h4> -->
<h3>Stochastic optimization for networked cyber-physical systems</h3>
<p>Cyber-physical systems (CPS) are systems in which communication, computation, and control are tightly coupled and interacting with the physical world. The complexity of CPS grows when a group of task-specific systems pool their resources and capabilities together to create a networked system offering more advanced functionality. 
A system of systems in this type is referred to a networked CPS. In order to properly manage networked CPS, most existing approaches assume that a CPS operator can obtain perfect (future) information of CPS all times, which requires perfect computation/communication/sensing from each subsystem. Leveraging stochastic optimization toolboxes, we have tackled online resource management problems (in different forms) for data centers, smart grids, and renewable-enabled next-generation communication systems, where the perfect information about the underlying systems is not available.</p>

<p><b>Related publications</b>:</p>
<ol>
<li><p><b>T. Chen</b>, X. Wang and G. Giannakis</b>, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8353.pdf">Cooling-Aware Energy and Workload Management in Data Centers via Stochastic Optimization</a>,’’ <i>IEEE Journal on Special Topics in Signal Processing</i>, Vol. 10, No. 2, pp. 402-415, Mar. 2016. </p>
</li>
<li><p><b>T. Chen</b>, Y. Zhang X. Wang, and G. Giannakis, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8373.pdf">Robust Workload and Energy Management for Sustainable Data Centers</a>,’’ <i>IEEE  Journal on Selected Areas in Communications</i>, Vol. 34, No. 3, pp. 651-664, Mar. 2016. </p>
</li>
<li><b>T. Chen</b>, A. Marques and G. Giannakis, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8400.pdf">DGLB: Distributed Stochastic Geographical Load Balancing over Cloud Networks</a>,’’ <i>IEEE Transactions on Parallel and Distributed Systems</i>, vol. 28, no. 7, pp. 1866-1880, July 2017. </p>
</li>
<li><p>X. Wang, <b>T. Chen</b>, X. Chen, X. Zhou and G. Giannakis, &lsquo;&lsquo;<a href="https://ieeexplore.ieee.org/document/7544445/">Dynamic Resource Allocation for Smart-Grid Powered MIMO Downlink Transmissions</a>,’’ <i>IEEE Journal on Selected Areas in Communications</i>, Vol. 34, No. 12, pp. 3354 - 3365, Dec. 2016. </p>
</li>
</ol>

<!-- <h2>Future Directions</h2>
<p>Moving forward, I will continue my research on developing scalable learning approaches for intelligent
systems including IoT. I will develop a research program that poses problems of practical
interest and addresses their theoretical challenges. Below is an outline of thrusts I aim to pursue. In my view, this challenge should be addressed by taking a holistic view of the entire information gathering and processing stack. Specifically, I would like to explore the following three directions through active collaboration with faculty members in related areas. </p> -->
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
