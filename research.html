<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="icon" href="cty.png" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Tianyi Chen 陈天翼</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="experience.html">Experience</a></div>
<div class="menu-item"><a href="awards.html">Awards</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research</h1>
<div id="subtitle"></div>
</div>
<h2>Research Overview</h2>  
  
	<div id="main">
	<br>
	<figure style="float:right; 
	 margin:0px 0px 15px 15px; 
	 padding: 5px;
	text-align: justify;">
    <img height="270" width="485" src="figures/overview.png" alt="" title="" > 
		<p></p>		
    <figcaption>
	<strong>Machine Learning and Optimization for Internet-of-Things </strong> 
	</figcaption>
	</figure>

<script type="text/javascript">
    function toggleAbstract(divid) {
      var x = document.getElementById(divid);
      if (x.style.display === "none") {
          x.style.display = "block";
      } else {
          x.style.display = "none";
      }
    }
  </script> 

    
<p>The past decade has witnessed a proliferation of connected devices and objects, where the notion
of Internet-of-Things (IoT) plays a central role in the envisioned technological advances. Conceptually
speaking, IoT foresees an intelligent system with ubiquitous smart devices [2]. Today, a number
of IoT applications have already brought major benefits to many aspects of our daily life. Despite
the popularity of IoT, several challenges must be addressed before embracing its full potential. In
this context, I work at the intersection of IoT and machine learning. My background is in statistical
signal processing, machine learning, optimization, and wireless communications. My current
research focuses on building fundamental connections between methodologies from the machine
learning and networking communities and developing inter-disciplinary approaches for IoT.
</p>
  
<p>My research has contributed answers to the following two intertwined questions.</p>
  
<p><b>(Q1)</b>: How can we scale up machine learning approaches for efficient IoT implementation?<br />
<b>(Q2)</b>: How learning advances can be leveraged to enhance resource allocation policies for IoT?</p>
  
<p>The overarching objective of my research is to wed state-of-the-art optimization and machine learning
tools with the emerging IoT paradigm, in a way that they can inspire and reinforce the development
of each other, with the ultimate goal of benefiting our daily life.</p>
  
<h2>Current Research</h2>
<!-- <p>Current ML systems are either centralized in a cloud, or distributed at the edge. In cloud platforms, data from the devices of end users, such as mobile phones, are transferred to the data centers which execute ML algorithms on CPU and GPU clusters. The extracted information is then transferred back to users’ devices. While cloud computing is rapidly expanding, <a href="http://ieeexplore.ieee.org/document/5559320/">recent work</a>  shows that the energy cost of transferring data between data centers and local devices can be a significant percentage of the total energy in cloud computing if the usage rate and size of data volume are large. Therefore, there has been an increasing interest to enable local inference capability in the end users’ devices. Local processing of raw data reduces energy and latency, and enhances privacy. In both platforms, there is a grand energy efficiency challenge as described next. </p> -->
<p>In the pursuit of the research goals, my contributions can be summarized into two major areas.</p>
		
<h3>Scale Up Machine Learning Approaches for IoT</h3>
<p>It is reported that US data centers consumed about 70 billion kilowatt-hours of electricity in 2014, representing 2% of the country’s total energy consumption. Indeed, the costs of power and cooling are becoming significant factors in the total expenditures of large-scale data centers. In particular, data transfer due to inter-chip, inter-board, inter-shelf and inter-rack communications within data centers and inter-site communications between data centers is one of the dominant energy costs. Results in this line of research have also been presented as a part of <a href="https://events.afcea.org/MILCOM18/Public/SessionDetails.aspx?FromPage=Sessions.aspx&SessionID=6712&SessionDateID=518">a tutorial</a> we delivered at MILCOM 2018.</p>
  
<h4> <li>Federated learning at the network edge</li> </h4>
Conventional machine learning approaches
require centralizing the users’ data on one machine or in a data center. Considering the
massive amount of IoT devices, centralized learning becomes computationally intractable, and rises
serious privacy concerns. To date, the widespread consensus is that besides data centers at the
cloud, future machine learning tasks have to be performed starting from the network edge, namely
mobile devices. This is the overarching goal of edge computing, also known as <a href="https://youtu.be/gbRJPa9d-VU">federated learning</a>. Towards this goal, my research efforts are centered on reducing the communication overhead
during the federated learning processes [3], and enhancing the robustness of learning under adversarial
attacks [16]. Relevant results have been published in top machine learning venues (NeurIPS
2018 and AAAI 2019). It is worth mentioning that our learning method with adaptive communication
mechanism [3] has been selected as the spotlight presentation in NIPS, which establishes
a provably reduced communication complexity in federated learning. Challenges of distributed
learning also lie in asynchrony and delay introduced by e.g., IoT mobility and heterogeneity. In this
context, we have developed algorithms for delayed online learning that can be run asynchronously
on edge devices; see our recent submission to AISTATS [15].</p>
<p><table class="imgtable"><tr><td>
<img src="figures/fedcomp.png" alt="need photo here" width="450" height="260" />&nbsp;</td>
<td align="left"></td></tr></table></p>
<p><b>Related publications</b>: </p>
<ol>
<li><p><b>T. Chen</b>, G. Giannakis, T. Sun and W. Yin, &lsquo;&lsquo;<a href="http://papers.nips.cc/paper/7752-lag-lazily-aggregated-gradient-for-communication-efficient-distributed-learning.pdf">LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning</a>,’’ <i>Proc. of Neural Information Processing Systems</i> (NeurIPS), Montreal, Canada, December 3-8, 2018. (Spotlight talk) </p>
</li> 
<li><p>L. Li, W. Xu, <b>T. Chen</b>, G. Giannakis, and Q. Ling, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1811.03761.pdf">&lsquo;&lsquo;RSA: Byzantine-Robust Stochastic Aggregation Methods for Distributed Learning from Heterogeneous Datasets,’’</a> <i>Proc. of the Assoc. for the Advanc. of Artificial Intelligence </i> (AAAI), Honolulu, Hawai, January 27-February 1, 2019. (Oral presentation)</p> 
</li>
<li><p>B. Li, <b>T. Chen</b>, and G. Giannakis, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1807.03205.pdf">Bandit Online Learning with Unknown Delays,</a>'' <i>Proc. of Intl. Conf. on Artificial Intell. and Stat.</i> (AISTATS), Naha, Japan, April 2019. </p>
</li>
</ol>

<h4> <li>Federated reinforcement learning over networked agents</li> </h4>
<p>From distributed machine learning to
distributed control, reinforcement learning will play a critical role in distributed management for IoT. Generalizing our theory and supervised learning algorithms [3], we have recently developed
exciting communication-efficient algorithms for distributed reinforcement learning [4]. </p>
	
<p><table class="imgtable"><tr><td>
<img src="figures/DRLtask.png" alt="need photo here" width="450" height="250" />&nbsp;</td>
<td align="left"></td></tr></table></p>
	
<p><b>Related publications</b>: </p>
<ol>
<li><p><b>T. Chen</b>, K. Zhang, G. B. Giannakis, and T. Ba&#351;ar, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1812.03239.pdf">Communication-Efficient Distributed Reinforcement Learning,’’</a> Submitted to  <i>Journal of Machine Learning Research</i>, December 2018. </p>
</li>  
</ol>
  
<h4><li>Scalable function approximation with unknown dynamics</li> </h4>
Function approximation emerges at the core of machine learning tasks such as regression, classification, dimensionality reduction, as well as reinforcement learning. Kernel methods exhibit well-documented performance in function
approximation. However, the major challenges of implementing existing methods to IoT come
from two sources: i) the “curse” of dimensionality in kernel-based learning; and, ii) the need to
track time-varying functions with unknown dynamics. In this context, we have developed a scalable
multi-kernel learning scheme to obtain the sought nonlinear learning function ‘on the fly.’
To further boost performance in unknown environments, we also developed an adaptive learning
scheme, which accounts for the unknown dynamics. So far, results in this direction have appeared
in the top artificial intelligence (AI) venue AISTATS 2018 [18] and the prestigious JMLR [19]. Our
scalable algorithm is uniquely capable of tracking learning functions in IoT environments with unknown
dynamics, and with analytical as well as empirical performance guarantees. </p>
<!-- <p><table class="imgtable"><tr><td>
<img src="figures/AdaRaker.png" alt="need photo here" width="450" height="250" />&nbsp;</td>
<td align="left"></td></tr></table></p> -->
<p><b>Related publications</b>: </p>
<ol>
<li><p>Y. Shen*, <b>T. Chen</b>*, and G. B. Giannakis, &lsquo;&lsquo;<a href="http://proceedings.mlr.press/v84/shen18a/shen18a.pdf">Online Ensemble Multi-Kernel Learning Adaptive to Nonstationary and Adversarial Environments</a>,’’ <i>Proc. of Intl. Conf. on Artificial Intell. and Stat.</i> (AISTATS), Lanzarote, Canary Islands, April 2018. </p>
</li>
<li><p>Y. Shen, <b>T. Chen</b>, and G. B. Giannakis, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1712.09983.pdf">Random Feature-Based Online Multi-Kernel Learning in Environments
with Unknown Dynamics</a>'' <i>Journal of Machine Learning Research</i>, conditional accepted, October 2018.</p>
</li>
</ol>
  
<h3>Rethink Resource Management for IoT via Machine Learning</h3>
<p>Optimally allocating limited computing and communication resources is a crucial task in IoT. To
tackle <b>(Q2)</b>, I have provided affirmative answers to the following intermediate questions:</p> 
  
<p><b> i)</b> can we learn from historical data to improve the existing resource management schemes; and,<br /> 
<b>ii)</b> can we develop resource management schemes when the underlying models are not known? <br /> </p>
  
<p>The key novelty here is innovative statistical and interactive learning tailored for resource management tasks in IoT. Results in this direction have been disseminated as part of <a href="http://globecom2018.ieee-globecom.org/program/tutorials#tut09">a tutorial</a> we co-presented at GLOBECOM 2018, and summarized in our <b>overview paper</b>.</p>
  
<!-- <p><b>Overview paper</b>:</p> -->
<p><b>T. Chen</b>, S. Barbarossa, X. Wang, G. Giannakis and Z.-L. Zhang, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1810.11613.pdf">Learning and Management for Internet-of-Things: Accounting for Adaptivity and Scalability</a>,’’ <i> Proceedings of the IEEE</i>, to appear, 2018.
 </p>

<h4><li>Statistical learning viewpoint of resource management</li></h4>
To date, most resource management schemes for IoT are based on a pure optimization viewpoint (e.g., the dual (sub)gradient method), which incur large queueing delays and slow convergence. From the vantage point of IoT, our fresh
idea is to leverage the abundant historical data collected by devices, and formulate the resource
management problem as an empirical risk minimization (ERM) — a central topic of statistical machine
learning research [1]. In this context, we have developed a fast convergent algorithm. By
cross-fertilizing advances of learning theory, we have also established the sample complexity of
learning a near-optimal resource management policy [7]. To boost performance in dynamic settings,
we further introduced a learn-and-adapt resource management framework [6], which capitalizes
on the following features: (f1) it learns from historical data using advanced statistical learning
tools; and, (f2) it efficiently adapts to IoT dynamics, and thus enables operational flexibility. Our
proposed algorithms have been published in top signal processing and network optimization
journals [6, 7], where we have analytically shown that this novel algorithmic design can provably
improve the emerging performance tradeoff by an order of magnitude. Preliminary results in this
direction have also played a key role in helping our group receive the NSF CCSS grant. To demonstrate
the impact of this work, we have applied it to mobile computing and smart grid tasks [13, 14].</p>
<p><table class="imgtable"><tr><td>
<img src="figures/statlearn.png" alt="need photo here" width="480" height="210" />&nbsp;</td>
<td align="left"></td></tr></table></p>
  
<p><b>Recent publications</b>:</p>
<ol>
<li><p><b>T. Chen</b>, A. Mokhtari, X. Wang, A. Ribeiro and G. Giannakis, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8422.pdf">Stochastic Averaging for Constrained Optimization with Application to Online Resource Allocation</a>,’’ <i>IEEE Transactions on Signal Processing</i>, vol. 65, no. 12, pp. 3078-3093, Jun. 2017. </p>
</li>
<li><p><b>T. Chen</b>, Q. Ling and G. Giannakis, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8437.pdf">Learn-and-Adapt Stochastic Dual Gradients for Network Resource Allocation</a>,’’<i>IEEE Transactions on Control of Network Systems</i>, vol. 5, no. 4, pp. 1941-1951, December 2018. </p>
</li>
<li><p>B. Li, <b>T. Chen</b>, X. Wang and G. Giannakis, &lsquo;&lsquo;<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8214260">Real-time Optimal Energy Management with Reduced Battery Capacity Requirements</a>,’’ <i>IEEE Transactions on Smart Grid</i>, to appear 2019. </p>
</li> 
<li><p>X. Chen, W. Ni, <b>T. Chen</b>, I. Collins, X. Wang and G. Giannakis, &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1804.07051.pdf">Multi-Timescale Optimization of
Network Function Virtualization for Service Chaining</a>,’’ <i>IEEE Transactions on Mobile Computing</i>, to appear 2019. </p>
</li>   
</ol>
<h4><li>Model-free interactive management for edge computing</li></h4>
Typically, solving resource allocation
problems necessitates knowledge of the models that map a resource allocation decision to its cost
or utility; e.g., the model that maps transmit-power to the bit rate in communication systems. However,
such models may not be available in IoT, because i) the utility function capturing e.g., service
latency or reliability in edge computing, can be hard to model; and, ii) even if modeling is possible,
IoT devices with limited resources may not afford the complexity of running sophisticated inference
algorithms. Hence, another important ingredient of my research is to account for the feedback
limited nature of resource allocation tasks in IoT. To account for physical constraints, we have considerably
generalized the interactive learning tools for unconstrained problems to solve challenging constrained resource allocation problems [5]. Tailored for edge computing scenarios, we further developed
a model-free online learning scheme [8], along with its bandit version [9]. These algorithms
come with provable performance guarantees, even when knowledge about the underlying system
models can be obtained only through repeated interactions with the environment. Results in this
direction were among the top 10 nominated for the best paper award in the 2017 IEEE Asilomar conference.</p>
<p><table class="imgtable"><tr><td>
<img src="figures/statlearn.png" alt="need photo here" width="450" height="210" />&nbsp;</td>
<td align="left"></td></tr></table></p>
<p><b>Related publications</b>:</p>
<ol>
<li><p><b>T. Chen</b>, Q. Ling and G. Giannakis, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8442.pdf">An Online Convex Optimization Approach to Proactive Network Resource Allocation</a>,’’ <i>IEEE Transactions on Signal Processing</i>, vol. 65, no. 24, pp. 6350-6364, Dec. 2017. </p>
</li>
<li><p><b>T. Chen</b>, Q. Ling, Y. Shen and G. Giannakis, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8498.pdf">Heterogeneous Online Learning for Thing-Adaptive Low-Latency Fog Computing in IoT</a>,’’ <i>IEEE Internet of Things Journal</i>, to appear 2018.</p>
</li>
<li><b>T. Chen</b> and G. Giannakis, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8470.pdf">Bandit Convex Optimization for Scalable and Dynamic IoT Management</a>,’’ <i>IEEE Internet of Things Journal</i>, to appear 2018. </p>
</li>
</ol>
  
<h4><li>Stochastic optimization for networked cyber-physical systems</li></h4>
<p>Networked cyber-physical systems (CPS) are systems in which communication, computation, and control are very tightly coupled and interacting in some way with the physical world. In order to properly design a successfully networked CPS, these three things must be considered simultaneously. In general, each component must be designed with the others in mind. For instance, a network operator cannot be designed assuming perfect information is available at all times, as this requires perfect computation/communication/sensing from other subsystems in the network.</p>

<p><b>Related publications</b>:</p>
<ol>
<li><p><b>T. Chen</b>, X. Wang and G. Giannakis</b>, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8353.pdf">Cooling-Aware Energy and Workload Management in Data Centers via Stochastic Optimization</a>,’’ <i>IEEE Journal on Special Topics in Signal Processing</i>, Vol. 10, No. 2, pp. 402-415, Mar. 2016. </p>
</li>
<li><p><b>T. Chen</b>, Y. Zhang X. Wang, and G. Giannakis, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8373.pdf">Robust Workload and Energy Management for Sustainable Data Centers</a>,’’ <i>IEEE  Journal on Selected Areas in Communications</i>, Vol. 34, No. 3, pp. 651-664, Mar. 2016. </p>
</li>
<li><b>T. Chen</b>, A. Marques and G. Giannakis, &lsquo;&lsquo;<a href="http://www.dtc.umn.edu/s/resources/spincom8400.pdf">DGLB: Distributed Stochastic Geographical Load Balancing over Cloud Networks</a>,’’ <i>IEEE Transactions on Parallel and Distributed Systems</i>, vol. 28, no. 7, pp. 1866-1880, July 2017. </p>
</li>
<li><p>X. Wang, <b>T. Chen</b>, X. Chen, X. Zhou and G. Giannakis, &lsquo;&lsquo;<a href="https://ieeexplore.ieee.org/document/7544445/">Dynamic Resource Allocation for Smart-Grid Powered MIMO Downlink Transmissions</a>,’’ <i>IEEE Journal on Selected Areas in Communications</i>, Vol. 34, No. 12, pp. 3354 - 3365, Dec. 2016. </p>
</li>
</ol>

<!-- <h2>Future Directions</h2>
<p>Moving forward, I will continue my research on developing scalable learning approaches for intelligent
systems including IoT. I will develop a research program that poses problems of practical
interest and addresses their theoretical challenges. Below is an outline of thrusts I aim to pursue. In my view, this challenge should be addressed by taking a holistic view of the entire information gathering and processing stack. Specifically, I would like to explore the following three directions through active collaboration with faculty members in related areas. </p> -->
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
