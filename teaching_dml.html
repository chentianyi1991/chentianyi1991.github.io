<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="icon" href="cty.png" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Tianyi Chen 陈天翼</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="people.html">People</a></div>    
<div class="menu-item"><a href="research.html">Research</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="teaching.html" class="current">Teaching</a></div>  
<div class="menu-item"><a href="experience.html">Experience</a></div>
<div class="menu-item"><a href="awards.html">Awards</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1> ECSE 4500/6500 Optimization and Learning in Distributed Systems </h1>
<div id="subtitle">previously "Distributed Systems and Sensor Networks'',&nbsp;  <a href="https://ecse.rpi.edu/~chent18/">Tianyi Chen</a>, Spring 2021
</div></div> 

<table class="imgtable"><tr><td>
<img src="figures/dml.png" alt="alt text" width="750 px" height="IMGLINKTARGET" />&nbsp;</td>
<td align="left"></td></tr></table>
<h2>Course Information </h2>
<p><b>Meeting Times</b>: Mon Thu: 10:10 AM - 11:30 AM <br />
<b>Webex link</b>: https://rensselaer.webex.com/meet/chent18<br />
<b>Instructor</b>: <a href="https://ecse.rpi.edu/~chent18/">Tianyi Chen</a>;&nbsp; <b>Office Hours</b>: Mondays at 5 pm – 6 pm<br />
<b>Teaching Assistant</b>: Xuefei Li;&nbsp; <b>Office Hours</b>: Wednesdays at 5 pm – 7 pm</p>

<h2>Course Description </h2>
<p>This course is neither about hardware implementation nor design of distributed systems. This is a course about algorithms and simulations of parallel and distributed optimization. The course covers parallel and distributed optimization algorithms and their analyses that are suitable for large-scale and distributed problems arising in machine learning and signal processing.</p>

<h2>Prerequisites </h2>
<p>This course is intended for graduate students and qualified undergraduate students with a strong mathematical and programming background. Undergraduate level coursework in linear algebra, calculus, probability, and statistics is suggested. A background in programming (e.g., Python and Matlab) is also necessary for the problem sets. A background in optimization and machine learning is also preferred. At RPI, the required courses are MATH 2010 and ESCE 2500, and the suggested courses are MATP4820 and <a href="teaching_intro_ml.html">ECSE 4840</a>, or permission by instructor. 
</p>
  
<h2>Student Learning Outcomes</h2>
<p>After taking the course, <b>undergraduate students</b> are expected to know:<br />
1.	how to formulate a distributed optimization problem from real-world learning, estimation, and control problems in distributed systems;<br />
2.	how to implement numerically stable algorithms to solve real-world engineering problems in distributed systems;<br />
3.	how to qualitatively evaluate the efficiency of a distributed algorithm for various applications in distributed systems.</p>

<p>In addition to the above, <b>graduate students</b> are also expected to know:<br />
1.	how to estimate the per-iteration complexity, namely, counting the number of arithmetic operations of a distributed optimization algorithm;<br />
2.	how to design and modify numerically stable algorithms to solve real-world engineering problems for various applications in distributed systems.</p>

<h2>Grading Criteria </h2>
<p> <b>Homework assignments</b>: total 6, 60% <br />
<b>Course project</b>: Proposal 5% + Presentation 10% + Report 25% <br />
<ul>ECSE 4500 – <b>Group project</b> (1-2 students). It can be either theoretic or experimental, with approval from the instructor. You are encouraged to combine your current research with your term project.</p>

<p>ECSE 6500 - <b>Individual project</b>. It can be either theoretic or experimental, with approval from the instructor. You are encouraged to combine your current research with your term project.</p></ul>

<h2>Optional References</h2>
<p>
1.	Dimitri P. Bertsekas and John N. Tsitsiklis, "<i><a href="http://web.mit.edu/dimitrib/www/pdc.html">Parallel and Distributed Computation: Numerical Methods</a></i>", Athena Scientific, 2015;<br />
2.	Stephen Boyd,  Neal Parikh, Eric Chu, Borja Peleato and Jonathan Eckstein, “<i><a href="http://www.nowpublishers.com/article/Details/MAL-016">Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers</a></i>,” Foundation and Trends Machine Learning, 2011;<br />
3.	Guanghui Lan, “<i><a href="http://pwp.gatech.edu/guanghui-lan/wp-content/uploads/sites/330/2019/08/LectureOPTML.pdf">Lectures on Optimization Methods for Machine Learning</a></i>,” preprint, 2019; <br />
4.	Ernest Ryu and Wotao Yin, “<i><a href="https://large-scale-book.mathopt.com/LSCOMO.pdf">A First Course in Large-Scale Optimization</a></i>,” preprint, 2020.
  </p>
  
<h2>Course Content </h2>
<p> 1.	Optimization basics and complexity measures<br />
2.	Basic machine learning models<br />
3.	Parallel and distributed architectures  <br />
4.	Synchronization issues in parallel and distributed algorithms <br />
5.	Communication aspects of parallel and distributed systems <br />
6.	Synchronous distributed algorithms<br />
<ul> a)	Distributed gradient descent<br />
b)	Distributed/local stochastic gradient descent<br />
c)	Distributed variance reduced stochastic gradient<br />
</ul>  
7.	Synchronous decentralized algorithms<br />
<ul>a)	Decentralized gradient descent<br />
b)	Decentralized ADMM<br />
c)	Decentralized stochastic gradient descent<br /></ul>  
8.	Asynchronous distributed and decentralized algorithms<br />
9.	Decentralized algorithms with time-varying topology <br />
10.	Applications in machine learning, signal processing and control<br />
<ul> a) Federated learning<br />
b) Distributed reinforcement learning<br />
c) Distributed power system state estimation<br />
d) Distributed parameter estimation in sensor networks
</p></ul>  


<div id="footer">
<div id="footer-text">
Page generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>  
</td>
</tr>
</table>
</body>
</html>
