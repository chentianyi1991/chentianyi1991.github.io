<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="google-site-verification" content="C4r0SmjTG14-FQFEYkVnw9U84lnlNDHyqvHTIWR2Bsw" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="icon" href="cty.png" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Tianyi Chen 陈天翼</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="people.html">People</a></div>  
<!-- <div class="menu-item"><a href="research.html">Research</a></div> -->
  <div class="menu-item"><a href="awards.html">Awards</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>    
<div class="menu-item"><a href="experience.html">Experience</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Tianyi Chen</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://chentianyi1991.github.io/"><img src="Photo_Tianyi.jpeg" alt="alt text" width="185px" height="250px" /></a>&nbsp;</td>
<td align="left">
<p> <b>Tianyi Chen</b></p>  
<a href="https://www.ecse.rpi.edu/people/faculty/tianyi-chen">Assistant Professor</a><br />
<a href="https://www.ecse.rpi.edu/">Department of Electrical, Computer, and Systems Engineering</a> <br />
<!--  <a href="https://idea.rpi.edu/"> Institute for Data Exploration and Applications</a> <br /> -->
<a href="http://www.rpi.edu/">Rensselaer Polytechnic Institute</a> </p>

Office: Room 6036, Jonsson Engineering Center <br />
Address: 110 8th Street, Troy, New York, 12180-3590 <br /> 
Tel: 518-276-3173 <br />  
<!-- Hometown: <a href="https://en.wikipedia.org/wiki/Shaoxing">Shaoxing</a>, Zhejiang, China <br />      -->
E-mail: chent18 [@] rpi.edu;&nbsp; chentianyi19 [@] gmail.com (<b>preferred</b>)</p>
  
<p><a href="https://scholar.google.com/citations?hl=en&user=kFwvv38AAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a><br />
<a href="https://www.researchgate.net/profile/Tianyi_Chen10">ResearchGate</a></p>    
</td></tr></table>
<h2>About me</h2>
<p>Tianyi Chen has been with Rensselaer Polytechnic Institute (RPI) as an assistant professor since August 2019. In RPI, I am also an active member in the Rensselaer-IBM Artificial Intelligence Research Collaboration (<a href="https://airc.rpi.edu/about/join-us">AIRC</a>). Prior to joining RPI, he received the doctoral degree from the University of Minnesota under the supervision of <a href="http://spincom.umn.edu/">Prof. Georgios B. Giannakis</a>.</p> 
  
<p>Dr. Chen is the inaugural recipient of IEEE Signal Processing Society Best PhD Dissertation Award in 2020, a recipient of NSF CAREER Award in 2021 and a recipient of Amazon Research Award in 2022. He is also a co-author of the Best Student Paper Award at the NeurIPS Federated Learning Workshop in 2020 and at IEEE ICASSP in 2021.</p>  
  
<p><b>Research interests</b>: theoretical foundations of optimization and machine learning, with a focus on their applications to emerging data processing and computing paradigms.</p>
<!-- <p>My Ph.D advisor is <a href="http://spincom.umn.edu/georgios/">Prof. Georgios B. Giannakis</a>.</p> -->
<!-- <p><tt>Notice: I am always looking for self-motivated <b>students</b> and <b>postdocs</b> working in the areas of <b>machine learning</b>, <b>optimization</b>, and <b>wireless networks</b>. Please drop me an email if you are interested.</tt></p>  -->
<h2>Education</h2>
<ul>
<li><p> <a href="https://ece.umn.edu/"> University of Minnesota</a>, Twin Cities, USA: Ph.D, Electrical and Computer Engineering, 2014.9 - 2019.6</p>
</li>
<li> <p> <a href="https://ece.umn.edu/"> University of Minnesota</a>, Twin Cities, USA: M.S., Electrical and Computer Engineering, 2014.9 - 2016.6</p>
<li><p> <a href="http://www.fudan.edu.cn/en/"> Fudan University</a>, China: B.S., Communication Science and Engineering, 2010.9 - 2014.7</p>
</li>
</ul>
  
 
	
<h2>Recent news</h2>
<ul>
 <li><p>Oct 2024: Two recent preprints on LLM post-training via bilevel/bi-objective optimization:
  <ul> 
   <a href="https://arxiv.org/abs/2410.07471">SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection</a>.</p>
   <a href="https://arxiv.org/abs/2410.15483">Mitigating Forgetting in LLM Supervised Fine-Tuning and Preference Learning</a>.</p>
</ul> </li>	
<li><p>Oct 2024: I am invited to present our work at <a href="https://find.engineering.cornell.edu/seminar/">The FIND Seminar</a>, Cornell University.<p>
</li>		
 <li><p>Oct 2023: I am serving as an Area Chair for <a href="http://aistats.org/aistats2025/"><b>AISTATS 2025</b></a> and <a href="https://iclr.cc/"><b>ICLR 2025</b></a>.<p>
</li>   	
 <li><p>Sep 2024: Three papers from our group have been accepted to <b>NeurIPS 2024</b>: 
   <a href="https://arxiv.org/pdf/2406.12774">Towards Exact Gradient-based Training on Analog In-memory Computing</a>.</p>
   <a href="https://arxiv.org/pdf/2406.10148">A Primal-Dual-Assisted Penalty Approach to Bilevel Optimization with Coupled Constraints</a>.</p>	 
   <a href="https://openreview.net/pdf?id=BmG3NgH5xu">FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning</a>.</p>	 	 
</li> 	
<li><p>Sep 2024: I presented a tutorial on multi-objective learning at the <b><a href="https://2024.ieeemlsp.org/">MLSP 2024</a></b> and a seminar on analog training at <b><a href="https://www.kcl.ac.uk/engineering">King's College London</a></b>.<p>
</li> 	
<!--  <li><p>Aug 2024: Two interesting ideas (one on bilevel optimization landscape and another on analog training) our group is exploring: 
  <ul> 
   <a href="https://arxiv.org/pdf/2406.12774">Towards Exact Gradient-based Training on Analog In-memory Computing</a>.</p>
   <a href="https://arxiv.org/pdf/2408.16087">Unlocking Global Optimality in Bilevel Optimization: A Pilot Study</a>.</p>
</ul> </li> 		 -->
 <li><p>Aug 2024: We are excited to receive a new collaborative <b>NSF project</b> on bilevel optimization for graph representation learning:
   <ul> <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2412486&HistoricalAwards=false">BLoG: A Bi-Level Optimization Framework for Learning Over Graphs</a>. </p>
</ul> </li>  	
 <li><p>Jul 2024: Our paper on using LLM for wireless symbol detection problems has been accepted to <a href="https://globecom2024.ieee-globecom.org/"<b>GLOBECOM 2024</b></a>:
<ul> <a href="https://arxiv.org/pdf/2409.00124">Leveraging Large Language Models for Wireless Symbol Detection via In-Context Learning</a>. </p>
</ul> </li>  		
 <li><p>May 2024: Two papers have been accepted to <b>ICML 2024</b> and one paper has been accepted to <b>JMLR</b>; see two preprints: 
  <ul> 
   <a href="https://arxiv.org/pdf/2402.06886">Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF</a>.</p>
   <a href="https://www.jmlr.org/papers/volume25/23-1287/23-1287.pdf">Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance</a>.</p>
</ul> </li> 		
<li><p>Apr 2024: We presented a tutorial at <a href="https://2024.ieeeicassp.org/tutorials/"><b>ICASSP 2024</b></a> on Learning with Multiple Objectives and Applications to Speech Processing; see <a href="https://lisha-chen.github.io/www/files/tutorial/aaai2024_tut.pdf">here</a>.</p>   
</li> 		
<li><p>Mar 2024: My Ph.D. student <b><a href="https://lisha-chen.github.io/">Lisha Chen</a> will join the <a href="https://www.hajim.rochester.edu/ece/index.html">Department of Electrical and Computer Engineering</a>, <a href="https://rochester.edu/">University of Rochester</a></b> in 2025 as a <b>tenure-track Assistant Professor</b>!<p>
</li>		
<li><p>Mar 2024: I am invited to present our work at the <a href="https://www.ece.rutgers.edu/">Department of Electrical and Computer Engineering</a>, Rutgers University.<p>
</li>	
<li><p>Feb 2024: We presented a tutorial at <a href="https://aaai.org/aaai-conference/aaai-24-tutorial-and-lab-list/#th08"><b>AAAI 2024</b></a> on Learning with Multiple Objectives and Applications to Reinforcement Learning; see <a href="https://lisha-chen.github.io/www/files/tutorial/aaai2024_tut.pdf">here</a>.</p>   
</li> 	
 <li><p>Feb 2024: Congrats to <a href="https://jenniferquanxiao.github.io/">Quan Xiao</a> on the <a href="https://ecse.rpi.edu/news/ecse-students-win-belsky-award-computational-sciences-and-engineering"> Belsky Award for Computational Sciences and Engineering</a>!</p>
  </li> 	
<!-- <li><p>Feb 2024: We will present two tutorials at <a href="https://aaai.org/aaai-conference/aaai-24-tutorial-and-lab-list/#th08"><b>AAAI 2024</b></a> and <a href="https://2024.ieeeicassp.org/tutorials/"><b>ICASSP 2024</b></a> on Learning with Multiple Objectives Beyond Bilevel Optimization. More details will follow.</p>   
</li> 		 -->
<li><p>Jan 2024: One paper has been accepted to <b>AISTATS 2024</b>: 
  <ul> <a href="https://arxiv.org/pdf/2401.12406.pdf">Enhancing In-context Learning via Linear Probe Calibration</a>. Congrats to <a href="https://mominabbas.github.io/">Momin</a>!</p>
</ul> </li>    	
 <li><p>Dec 2023: Three of our papers were accepted in <b>ICASSP 2024</b>; see e.g., <a href="https://arxiv.org/pdf/2401.06980.pdf">here</a>.</p>
  </li> 		
<li><p>Oct 2023: I am invited to present our work at the <a href="https://ece.northeastern.edu/">Department of Electrical and Computer Engineering</a>, Northeastern University.<p>
</li> 
 <li><p>Oct 2023: I am invited to serve as an Area Chair for <a href="http://aistats.org/aistats2024/"><b>AISTATS 2024</b></a> and <a href="https://cpal.cc/"><b>CPAL 2024</b></a>.<p>
</li>     	
 <li><p>Sep 2023: Two papers have been accepted to <b>NeurIPS 2023</b>: 
  <ul> <a href="https://arxiv.org/pdf/2305.20057">Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance</a>.</p>
   <a href="https://arxiv.org/pdf/2306.02422.pdf">A Generalized Alternating Method for Bilevel Learning under the Polyak-Lojasiewicz Condition</a>.</p>
</ul> </li> 	
<li><p>Sep 2023: I am invited to present our work on multi-objective learning at a departmental seminar from <a href="https://engineering.lehigh.edu/ise">Department of Industrial and Systems Engineering</a>, Lehigh University.<p>
</li> 		
<li><p>Aug 2023: Our paper on lazy query for zeroth-order optimization has been accepted in <b>TSP</b>: 
  <ul> <a href="https://arxiv.org/pdf/2206.07126.pdf">Lazy Queries Can Reduce Variance in Zeroth-order Optimization</a>.</p>
</ul> </li> 
<li><p>Jun 2023: I am invited to present our work on bilevel optimization at <a href="https://www.siam.org/conferences/cm/conference/op23"><b>SIAM Conference on Optimization</b></a> and <a href="https://coral.ise.lehigh.edu/~mopta/"><b>MOPTA</b></a>.<p>
</li> 	
<li><p>Apr 2023: One paper on the inexact penalization method for bilevel optimization has been accepted in <b>ICML 2023</b>: 
  <ul> <a href="https://arxiv.org/pdf/2302.05185.pdf">On Penalty-based Bilevel Gradient Descent Method</a>. Congrats to <a href="https://hanshen95.github.io/">Han</a>!</p>
</ul> </li>  
<li><p>Apr 2023: Our paper on linear speedup analysis of the popular <b>A3C</b> algorithm in RL has been accepted in <b>TSP</b>: 
  <ul> <a href="https://arxiv.org/pdf/2012.15511.pdf"> Asynchronous Advantage Actor-Critic: Non-asymptotic Analysis and Linear Speedup</a>.</p>
</ul> </li>   
 <li><p>Feb 2023: Our paper on ensemble methods for bilevel learning was accepted in <b>ICASSP 2023</b>. Congrats to <a href="https://lisha-chen.github.io/">Lisha</a>!</p>
  </li> 
 <li><p>Feb 2023: I am invited to present our work on bilevel optimization for distributed learning at <a href="https://encore.ucsd.edu/encore-ita-2023/"><b>EnCORE workshop</b></a> at <b>ITA 2023</b>.<p>
</li>    
<li><p>Jan 2023: One paper has been selected as an <b>oral</b> paper (notable-top-5%) in <b>ICLR 2023</b>: 
  <ul> <a href="https://arxiv.org/pdf/2210.12624.pdf">Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Stochastic Approach</a>. Congrats to <a href="https://heshandevaka.github.io/">Heshan</a>!</p>
</ul> </li>   	
<li><p>Jan 2023: Two papers have been accepted to <b>AISTATS 2023</b>: 
  <ul> <a href="https://arxiv.org/pdf/2211.07096.pdf">Alternating Implicit Projected SGD and Its Efficient Variants for Equality-constrained Bilevel Optimization</a>. Congrats to <a href="https://jenniferquanxiao.github.io/">Quan</a>!</p>
   <a href="https://proceedings.mlr.press/v206/shen23b/shen23b.pdf">Distributed Offline Policy Optimization Over Batch Data</a>. Congrats to <a href="https://hanshen95.github.io/">Han</a>!</p>
</ul> </li>     	
<li><p>Dec 2022: With Atlas Wang, we will present a tutorial at <b>ICASSP 2023</b> on Bilevel Optimization and Its Applications to Machine Learning.</p>   
</li> 	
 <li><p>Nov 2022: We are honored to receive the Faculty Research Gifts from <a href="https://research.cisco.com/"> <b>Cisco Research</b></a>.</p>  
</li>       
 <li><p>Nov 2022: We are happy to receive several IBM-RPI AI research projects, part of the   <a href="http://ibm.biz/AIHorizons"> <b>IBM AI Horizons Network</b></a>.</p>  
</li>       	
<li><p>Oct 2022: Our monograph with Osvaldo's group on <a href="https://arxiv.org/pdf/2210.02515.pdf">Meta Learning and Applications to Communications</a> is published in <b>Foundations and Trends in Signal Processing</b>.</p>  
</li>      
 <li><p>Oct 2022: We are honored to receive the <a href="https://www.amazon.science/research-awards/program-updates/fall-2021-and-winter-2022-amazon-research-awards-recipients-announced"> <b>Amazon Research Award - AWS AI</b></a>.</p>  
</li>      
 <li><p>Sep 2022: Two papers have been accepted to <b>NeurIPS 2022</b> with one being selected as an <b>oral</b> presentation: 
  <ul> <a href="https://arxiv.org/abs/2206.13482">Understanding Benign Overfitting in Gradient-based Meta Learning</a>. Congrats to <a href="https://lisha-chen.github.io/">Lisha</a>!</p>
   <a href="https://arxiv.org/pdf/2206.10414.pdf"> A Single-Timescale Analysis For Stochastic Approximation With Multiple Coupled Sequences</a> (oral). Congrats to <a href="https://hanshen95.github.io/">Han</a>!</p>
</ul> </li>   
 <li><p>Sep 2022: I am invited to serve as an Area Chair for <a href="https://aistats.org/aistats2023/index.html"><b>AISTATS 2023</b></a>.<p>
</li>     
 <li><p>Aug 2022: We have organized a Cross-Community Federated Learning (<b>CrossFL</b>) workshop at <b><a href="https://mlsys.org/">MLSys 2022</a></b>; see the recording [<a href="https://www.youtube.com/watch?v=0mV85uGMpXA">link</a>].</p>
</li>  
<li><p>May 2022: Our paper has been accepted to <b>ICML 2022</b>! Congrats to the teamwork among <a href="https://mominabbas.github.io/">Momin</a>, <a href="https://jenniferquanxiao.github.io">Quan</a> and <a href="https://lisha-chen.github.io/">Lisha</a>!
  <ul> <a href="https://arxiv.org/pdf/2206.03996.pdf">Sharp-MAML: Sharpness-Aware Model-Agnostic Meta Learning</a>.</p>
</ul>  
</li>    
<!-- <li><p>May 2022: Our paper on benign overfitting for federated learning has been accepted to <b>EUSIPCO 2022</b>! Congrats to <a href="https://lisha-chen.github.io/">Lisha</a>! 
</li>      -->
 <li><p>Mar 2022: Congrats to <a href="https://lisha-chen.github.io/">Lisha</a> on the inaugural <a href="https://ecse.rpi.edu/news/ecse-graduate-students-receive-soes-belsky-award-computational-sciences-and-engineering">
  Belsky Award for Computational Sciences and Engineering</a>!</p>
  </li>   
<li><p>Jan 2022: Two papers have been accepted to <b>AISTATS 2022</b> with one being selected as an <b>oral</b> presentation: 
  <ul> <a href="https://arxiv.org/pdf/2102.04671">A Single-Timescale Method for Stochastic Bilevel Optimization</a> (oral).</p>
   <a href="https://arxiv.org/pdf/2203.03059.pdf">Is Bayesian Model-Agnostic Meta Learning Better than Model-Agnostic Meta Learning, Provably?</a> (poster).</p>
</ul> </li>   
 <li><p>Jan 2022: Our paper with Prof. <a href="https://ecse.rpi.edu/people/faculty/ali-tajer">Ali Tajer</a>'s group was accepted in <b>ICASSP 2022</b> on federated bandit learning with uncoordinated exploration.</p>
  </li> 
<li><p>Dec 2021: I will co-organize a workshop at <b><a href="https://mlsys.org/">MLSys 2022</a></b> with <b><a href="https://sites.google.com/site/pinyuchenpage">Pin-Yu Chen</a></b>, <b><a href="https://www.andrew.cmu.edu/user/cjoewong/">Carlee Joe-Wong</a></b>, and <b><a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-baracald">Nathalie Baracaldo</a></b> on
   <ul>Cross-Community Federated Learning: Algorithms, Systems and Co-designs. [<a href="https://crossfl2022.github.io/">link</a>]</p>
</ul>
</li>    
<li><p>Nov 2021: I will co-present a tutorial at <b>ICASSP 2022</b> with Prof. <b><a href="https://www.nms.kcl.ac.uk/osvaldo.simeone/index.htm">Osvaldo Simeone</a></b> on
   <ul><a href="https://2022.ieeeicassp.org/tutorials.php#tut14">Meta Learning and Applications to Communications</a>.</p>
</ul>
</li>  
<li><p>Sep 2021: Two papers have been accepted to <b>NeurIPS 2021</b> with one being selected as a <b>spotlight</b> presentation: 
  <ul> <a href="https://proceedings.neurips.cc/paper/2021/file/d4dd111a4fd973394238aca5c05bebe3-Paper.pdf">Closing the Gap: Tighter Analysis of Alternating Stochastic Gradient Methods for Bilevel Problems</a> (spotlight).</p>
   <a href="https://arxiv.org/pdf/2110.15122.pdf"> CAFE: Catastrophic Data Leakage in Vertical Federated Learning</a> (poster).</p>
</ul> </li>  
<li><p>Sep 2021: Our paper was accepted in <b>IEEE Trans on Pattern Analysis and Machine Intelligence</b>: 
 <ul> <a href="https://arxiv.org/pdf/2002.08537.pdf">Adaptive Temporal Difference Learning with Linear Function Approximation</a>.<p>
</ul> </li>  
<li><p>Sep 2021: I am invited to the Panel Session of the "<b><a href="https://distributedml.org/speakers/">Distributed ML workshop</a></b>'' co-located with ACM CoNEXT 2021; see the <a href="https://distributedml.org/program/">program</a>.</p> 
</li>    
<li><p>Sep 2021: With <b><a href="https://sites.google.com/site/deryamalak/">Derya Malak</a></b>, we are organizing a workshop on "<b><a href="https://wiopt.online/workshops/track/CCDWN">Wireless Distributed Computing and Learning</a></b>'' at WiOPT 2021.</p> 
</li>   
 <li><p>Aug 2021: I am excited to collaborate with <b><a href="https://sites.google.com/view/rongjielai/home?authuser=0">Rongjie Lai</a></b> (RPI) and <b><a href="https://jiechenjiechen.github.io/">Jie Chen</a></b> (MIT-IBM AI Lab) for the <b><a href="https://www.nsf.gov/pubs/2021/nsf21561/nsf21561.htm">NSF SCALE MoDL</a></b> Program on:
   <ul> <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2134168&HistoricalAwards=false">Representation Learning via Variational Mean Field Theory</a>. </p>
</ul> </li>  
<li><p>Jul 2021: Our paper was accepted in <b>IEEE Transactions on Signal Processing</b>: 
<ul> <a href="https://arxiv.org/pdf/2002.11360.pdf">LASG: Lazily Aggregated Stochastic Gradients for Communication-Efficient Distributed Learning</a>.<p>
</ul> </li>     
<li><p>Jun 2021: Our paper was recognized as the <a href="https://twitter.com/ieeeICASSP/status/1403454543820697602"><b>Outstanding Student Paper Award</b></a> for <b>ICASSP 2021</b>: 
  <ul> <a href="https://ieeexplore.ieee.org/document/9414369">A Stochastic Compositional Optimization Method with Applications to Meta Learning</a>. [<a href="https://ww3.math.ucla.edu/dr-yuejiao-sun-receives-outstanding-student-paper-award-from-icassp2021/">UCLA Math News</a>]</p>
</ul> </li>  
<li><p>Jun 2021: Two papers were accepted in <b>IEEE Transactions on Signal Processing</b>: 
<ul> <a href="https://arxiv.org/pdf/2008.10847.pdf"> Solving Stochastic Compositional Optimization is Nearly As Easy As Solving Stochastic Optimization</a>.</p>
  <a href="https://arxiv.org/pdf/2009.11146.pdf"> Byzantine-Resilient Decentralized TD Learning with Linear Function Approximation</a>.</p>
</ul> </li>    
 <li><p>May 2021: I am honored to receive the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2047177&HistoricalAwards=false"> <b>NSF CAREER Award</b></a> on: 
  <ul> The Co-design of Distributed Machine Learning Algorithms and Wireless Systems. [<a href="https://news.rpi.edu/content/2021/06/22/intelligence-sharing-tools-will-enable-smarter-devices">RPI News</a>] </p>
</ul> </li>  
      <li><p>Apr 2021: Our paper was accepted in <b>IEEE Transactions on Control of Network systems</b>: 
  <ul>  <a href="https://arxiv.org/pdf/1812.03239.pdf"> Communication-Efficient Policy Gradient Methods for Distributed Reinforcement Learning</a>.</p>
</ul> </li>  
 <li><p>Feb 2021: I am organizing the special session ``<a href="https://cmsworkshops.com/Asilomar2021/Papers/ViewSession.asp?Sessionid=1027"><b>Reinforcement learning over networks</b></a>'' at <b>Asilomar 2021</b>.</p> 
</li>     
 <li><p>Feb 2021: New paper on the first single-timescale SGD algorithms for stochastic bilevel optimization: 
  <ul> <a href="https://arxiv.org/pdf/2102.04671.pdf"> A Single-Timescale Stochastic Bilevel Optimization Method</a>.</p>
</ul> </li>   
      <li><p>Jan 2021: Our papers were accepted in <b>ICASSP 2021</b> on meta learning and reinforcement learning.</p>
  </li>
    <li><p>Jan 2021: Our paper was accepted in <b> AISTATS 2021</b>: 
  <ul> <a href="https://arxiv.org/pdf/2012.15469.pdf"> CADA: Communication-Adaptive Distributed Adam</a>.</p>
</ul> </li>    
    <li><p>Jan 2021: My Ph.D dissertation was recognized as the inaugural <a href="https://signalprocessingsociety.org/community-involvement/award-recipients"> <b>IEEE SPS Best PhD Dissertation Award</b></a>: 
  <ul> Efficient Methods for Distributed Machine Learning and Resource Management in IoT. [<a href="https://ece.umn.edu/alumnus-tianyi-chen-receives-ieee-sps-best-dissertation-award/">UMN News</a>] [<a href="https://www.ecse.rpi.edu/news/ecse-prof-tianyi-chen-inaugural-recipient-ieee-signal-processing-society-sps-best-phd">RPI ECSE News</a>] [<a href="https://news.rpi.edu/approach/2020/12/21">RPI News</a>] </p>
</ul> </li>
<li><p>Dec 2020: New paper on linear speedup analysis of the popular <b>A3C</b> algorithm in RL: 
  <ul> <a href="https://arxiv.org/pdf/2012.15511.pdf"> Asynchronous Advantage Actor Critic: Non-asymptotic Analysis and Linear Speedup</a>.</p>
</ul> </li>   
  <li><p>Dec 2020: Our paper was accepted in <b>AAAI 2021</b>: 
  <ul> <a href="aaai.pdf"> Decentralized policy gradient descent ascent for safe multi-agent reinforcement learning</a>.</p>
</ul> </li>  
  <li><p>Dec 2020: Our paper was recognized as the <b>Best Student Paper Award</b> at <b>NeurIPS 2020 workshop</b> on Federated Learning:  
  <ul> <a href="https://arxiv.org/pdf/2012.12420.pdf"> Hybrid Federated Learning: Algorithms and Implementation</a>. [<a href="https://ww3.math.ucla.edu/professor-yin-and-team-wins-best-student-paper-award-on-scalability-privacy-and-security-in-federated-learning/">UCLA Math News</a>]</p>
<!--    <a href="https://opt-ml.org/papers/2020/paper_21.pdf"> CADA: Communication-Adaptive Distributed Adam</a>.</p>  -->
</ul> </li>  
<li><p>Sep 2020: Our paper was accepted in <b>IEEE Trans on Pattern Analysis and Machine Intelligence</b>: 
  <ul> <a href="https://ieeexplore.ieee.org/abstract/document/9238427"> Lazily Aggregated Quantized Gradient Innovation for Communication-Efficient Federated Learning</a>.</p>
</ul> </li>  
<!--  <li><p>Aug 2020: I will teach <b>ECSE 4962 Introduction to Machine Learning</b> in Fall 2020.</p>
</li>   -->
<li><p>Aug 2020: New paper on simple stochastic algorithm for learning with compositional structures (e.g., meta learning and RL): 
  <ul> <a href="https://arxiv.org/pdf/2008.10847.pdf">Solving Stochastic Compositional Optimization is Nearly as Easy as Solving Stochastic Optimization</a>.<p>
</ul></li>    
<!-- <li><p>Aug 2020: I am invited to serve as a program committee member for <a href="https://iclr.cc/">ICLR 2021</a>.<p>
</li>    -->
<!-- <li><p>Jul 2020: New paper presented in <a href="http://federated-learning.org/fl-icml-2020/">ICML Workshop on Federated Learning for User Privacy and Data Confidentiality 2020</a>: 
  <ul> <a href="https://arxiv.org/pdf/2007.06081.pdf">VAFL: a Method of Vertical Asynchronous Federated Learning</a>.<p>
</ul></li>   -->
 <li><p>May 2020: I am invited to serve as a program committee member for <a href="https://icml.cc/Conferences/2020">ICML 2020</a>, <a href="https://nips.cc/">NeurIPS 2020</a> and <a href="https://iclr.cc/">ICLR 2021</a>.<p>
</li>     
<li><p>May 2020: New paper on Byzantine-robust federated learning has been accepted in <b>IEEE Transactions on Signal Processing</b>: 
  <ul> <a href="https://arxiv.org/pdf/1912.12716.pdf">Federated Variance-Reduced Stochastic Gradient Descent with Robustness to Byzantine Attacks</a>.<p>
</ul></li> 
 <li><p>Mar 2020: I am organizing the special session ``<a href="https://www2.securecms.com/Asilomar2020/Papers/PublicSessionIndex3.asp?Sessionid=1005"><b>Reinforcement learning for communication systems</b></a>'' at <b>Asilomar 2020</b>.</p> 
</li>     
<!--  <li><p>Feb 2020: Presented our work on communication-efficient federated learning at George Mason University ECE Seminar.<p>
</li>   -->
<!--  <li><p>Feb 2020: I am invited to serve as a program committee member for <a href="https://icml.cc/Conferences/2020">ICML 2020</a>.<p>
</li>    -->
<li><p>Feb 2020: New paper on adaptive temporal-difference learning and <a href="https://github.com/CoeusMaze/Adaptive-Temporal-Difference-Learning"><b>[Python code]</b></a>: 
  <ul> <a href="https://arxiv.org/pdf/2002.08537.pdf">Adaptive Temporal Difference Learning with Linear Function Approximation</a>.<p>
</ul></li> 
<li><p>Jan 2020: Our paper on variance-reduced robust stochastic gradient descent has been accepted in <b>ICASSP 2020</b>. </p>
</li>  
<li><p>Jan 2020: I am invited to serve as a program committee member for <a href="https://www.sigmobile.org/mobihoc/2020/">ACM MobiHoc 2020</a>.</p> 
</li>    
<!-- <li><p>Dec 2019: New paper on variance-reduced robust stochastic gradient descent: 
  <ul> <a href="https://arxiv.org/pdf/1912.12716.pdf">Federated Variance-Reduced Stochastic Gradient Descent with Robustness to Byzantine Attacks</a>.</p>
</ul> </li>    -->
<li><p>Dec 2019: Presented our work on Lazily Aggregated Quantized Gradients at <b>NeurIPS</b> 2019: <a href="https://papers.nips.cc/paper/8598-communication-efficient-distributed-learning-via-lazily-aggregated-quantized-gradients.pdf"><b>[Paper]</b></a> and <a href="https://www.dropbox.com/s/hliklys5xjyzspi/NeurIPS_Sunposter.pdf?dl=0"><b>[Poster]</b></a>
</li>  
<li><p>Oct 2019: Our paper has been accepted in <b>IEEE Transactions on Signal Processing</b>: 
  <ul> <a href="https://ieeexplore.ieee.org/document/8882321">Secure Mobile Edge Computing in IoT via Collaborative Online Learning</a>.</p>
</ul> </li>    
<li><p>Sept 2019: Our paper was accepted at <b>NeurIPS</b> 2019: 
  <ul> <a href="https://arxiv.org/pdf/1909.07588.pdf">Communication-Efficient Distributed Learning via Lazily Aggregated Quantized Gradients</a>. <a href="https://github.com/sunjunaimer/LAQ"><b>[Python code]</b></a></p>
</ul> </li>
<!-- <li><p>Aug 2019: I will teach <b>ECSE 6510 Introduction to Stochastic Signal and Systems</b> in Fall 2019.</p>
</li>   -->
<li><p>Aug 2019: I am excited to join Department of Electrical, Computer, and Systems Engineering at <a href="http://www.rpi.edu/">Rensselaer Polytechnic Institute</a>.</p>
</li>
<!-- <li><p>July 2019: Presented our work on communication-efficient collaborative distributed learning at <a href="https://damo.alibaba.com/">The Alibaba Damo Academy</a>, Seattle, WA. </p>
</li>  
<li><p>June 2019: Presented our work at <a href="https://www.starkey.com/">Starkey Hearing Technologies</a> sponsored by the IEEE Twin Cities Section Chapter meeting. </p>
</li> 
<!-- <li><p>May 2019: Officially obtained my PhD from University of Minnesota, Twin Cities. </p>
</li>    -->
<!--<li><p>Jan 2019: Visited School of Industrial Engineering at Purdue University,
West Lafayette. Host: <a href="https://engineering.purdue.edu/~gscutari/">Gesualdo Scutari</a>.</p>
</li>    
<li><p>Jan 2019: Defended my doctoral oral exam entitled "Efficient Methods for Distributed Learning and Resource Management in the IoT." 
  <ul>Thanks my committee members: Profs. Arindam Banerjee (chair), Georgios Giannakis (advisor), Mingyi Hong, Na Li, and Zhi-Li Zhang!</p>
</ul></li>     
<li><p>Dec 2018: Our paper on delayed online learning was accepted at <b>AISTATS</b> 2019: 
  <ul> <a href="http://proceedings.mlr.press/v89/li19d/li19d.pdf">Bandit Online Learning with Unknown Delays</a>.</p>
</ul> </li>  
<li><p>Dec 2018: Our paper was accepted for publication at <b>Journal of Machine Learning Research</b>: 
 <ul> <a href="http://www.jmlr.org/papers/volume20/18-030/18-030.pdf">Random Feature-based Online Multi-kernel Learning in Environments with Unknown Dynamics</a>. </p>
</ul> </li>  
<li><p>Dec 2018: We gave a tutorial at <b>GLOBECOM</b> 2018: <a href="http://globecom2018.ieee-globecom.org/program/tutorials#tut09">Interactive Optimization and Learning for IoT</a>. </p>
</li>
<li><p>Dec 2018: New paper on distributed reinforcement learning: 
  <ul> <a href="https://arxiv.org/abs/1812.03239">Communication-Efficient Distributed Reinforcement Learning</a>.</p>
</ul> </li>  
<li><p>Dec 2018: I gave <b>spotlight</b> presentation on communication-efficient machine learning for IoT at <b>NeurIPS</b> 2018. </p>
<ul>Please check out our spotlight <a href="https://www.videoken.com/embed/-pywxDhaBzc?tocitem=8"><b>talk</b></a>, <a href="https://www.dropbox.com/s/m80ytd2kqn4b72p/poster_nips.pdf?dl=0"><b>poster</b></a>, and <a href="http://papers.nips.cc/paper/7752-lag-lazily-aggregated-gradient-for-communication-efficient-distributed-learning.pdf"><b>paper</b></a>. </p>
</ul></li>
<li><p>Nov 2018: Our paper was accepted for publication at <b>Proceedings of the IEEE</b>: 
<ul><a href="http://www.dtc.umn.edu/s/resources/spincom8516.pdf">Learning and Management for Internet-of-Things: Accounting for Adaptivity and Scalability</a>. </p>
</ul> </li>  
<li><p>Nov 2018: Our paper on Byzantine SGD for learning from heterogeneous datasets was accepted at <b>AAAI</b> 2019.</p>
<ul><a href="https://arxiv.org/pdf/1811.03761.pdf">RSA: Byzantine-Robust Stochastic Aggregation Methods for Distributed Learning from Heterogeneous Datasets</a>. </p>
</ul> </li>     
<li><p>Oct 2018: We gave a tutorial at <b>MILCOM</b> 2018: <a href="https://events.afcea.org/MILCOM18/Public/SessionDetails.aspx?FromPage=Sessions.aspx&SessionID=6712&SessionDateID=518">Resilient	and	Scalable	Interactive Learning</a>. </p>
</li>  
<li><p>Oct 2018: Attended the <a href="http://www.asilomarsscconf.org/">Asilomar Conference</a> on Signals, Systems, and Computers.</p>
</li>  
<li><p>Oct 2018: Attended Annual <a href="http://allerton.csl.illinois.edu//">Allerton Conference</a> on Communication, Control, and Computing.</p>
</li> -->
<!-- <li><p>Sept 2018: Visited Coordinated Science Laboratory, University of Illinois at Urbana-Champaign. Host: <a href="http://tamerbasar.csl.illinois.edu/">Tamer Başar</a>. </p>
</li>  
<li><p>Sept 2018: Our paper was accepted for <b>spotlight</b> presentation at <b>NeurIPS</b> 2018: 
  <ul> <a href="https://arxiv.org/pdf/1805.09965.pdf">LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning</a>.</p>
</ul> </li>
<li><p>Aug 2018: Our paper was accepted for publication at <b>IEEE Internet-of-Things Journal</b>: 
  <ul> <a href="http://www.dtc.umn.edu/s/resources/spincom8498.pdf">Heterogeneous Online Learning for "Thing-Adaptive" Low-Latency Fog Computing in IoT</a>. </p>
</ul> </li>   -->
<!-- <li><p>Aug 2018: Presented our work on online learning and management for edge computing in IoT at Huawei University Day. </p>
</li>   -->
<!-- <li><p>May 2018: New paper on online learning for cyber-security in IoT: 
  <ul> <a href="https://arxiv.org/pdf/1805.03591.pdf">Secure Mobile Edge Computing in IoT via Collaborative Online Learning</a>. </p>
</ul> </li>
<li><p>May 2018: Our paper was accepted for publication at <b>IEEE Internet-of-Things Journal</b>: 
  <ul> <a href="https://ieeexplore.ieee.org/abstract/document/8362620">Bandit Convex Optimization for Scalable and Dynamic IoT Management</a>. </p>
</ul> </li>   -->
<!-- <li><p>May 2018: New paper on distributed machine learning: 
  <ul> <a href="https://arxiv.org/pdf/1805.09965.pdf">LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning</a>.</p>
</ul> </li> -->
<!-- <li><p>April 2018: Our paper on interactive learning in nonstationary environments was presented at <b>AISTATS</b> 2018: 
  <ul> <a href="http://proceedings.mlr.press/v84/shen18a/shen18a.pdf">Online Ensemble Multi-kernel Learning Adaptive to Non-stationary and Adversarial Environments</a>.</p>
</ul> </li>
<li><p>April 2018: Presented our work on model-free interactive learning for IoT management at <a href="https://2018.ieeeicassp.org/">ICASSP 2018</a>. </p>
</li>
<li><p>Feb 2018: Presented our work on bandit learning for IoT management at 2018 <a href="http://ita.ucsd.edu/workshop/18/talks.php#talk2">ITA Graduation Day 2018</a>. </p>
</li>
<li><p>Jan 2018: Visited Department of Mathematics, University of California, Los Angeles. Host: <a href="http://www.math.ucla.edu/~wotaoyin/">Wotao Yin</a>. </p>
</li>   -->
<!-- <li><p>Jan 2018: New paper on scalable kernel learning: 
 <ul>  <a href="https://arxiv.org/pdf/1712.09983.pdf">Random Feature-based Online Multi-kernel Learning in Environments with Unknown Dynamics</a>. </p>
</ul> </li>   -->
</ul>
</div>

<h2>Sponsors</h2>	
	
<p>
<div style="text-align: center;"><img class="displayed" src="figures/funds2.jpg" width="600 px" align="center"; /></div>
</p>
 
</div>
</div>

<div id="footer">
<div id="footer-text">
Page generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
<script type="text/javascript">
var sc_project=10867279; 
var sc_invisible=1; 
var sc_security="cab9fef6"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/10867279/0/cab9fef6/1/"
alt="Web Analytics"></a></div></noscript>
</table>
</body>
</html>
